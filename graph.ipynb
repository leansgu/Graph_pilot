{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE\n",
    " DATALOADER --> GRAPH CONSTRUCTION --> GNN --> EXPLAIN "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUB (Dataset):\n",
    "    '''\n",
    "    parts \n",
    "        part_locs \n",
    "        parts (names)\n",
    "    images \n",
    "        200 classes\n",
    "    bounding box  (for cropping images)\n",
    "    classes.txt\n",
    "    train_test_split.txt\n",
    "    image_class_labels.txt\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.root_dir = root_dir # D:\\TorchProject\\dataset\\cub\\CUB_200_2011\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir,'images')\n",
    "        self.image_mapping = self._load_image_mapping()\n",
    "        self.label_mapping = self._load_label_mapping()\n",
    "        self.part_locs_mapping = self._load_part_locs_mapping()\n",
    "        self.part_name_mapping = self._load_part_name_mapping()\n",
    "        self.bounding_box_mapping = self._load_bounding_box()\n",
    "        #self.classes_name = self._load_class_names()\n",
    "    \n",
    "    # def _load_class_names(self):\n",
    "    #     class_names = {}\n",
    "    #     classes_file = os.path.join(self.root_dir,'classes.txt')\n",
    "    #     with open(classes_file, 'r') as file:\n",
    "    #         for line in file:\n",
    "    #             class_id, class_name = line.strip().split()\n",
    "    #             if class_id not in class_names:\n",
    "    #                 class_names[class_id] = {}\n",
    "    #             class_names[class_id] = class_name\n",
    "    #     return class_names\n",
    "\n",
    "    def _load_bounding_box(self):\n",
    "        bounding_box = {}\n",
    "        box_file = os.path.join(self.root_dir, 'bounding_boxes.txt')\n",
    "        with open(box_file, 'r') as file:\n",
    "            for line in file:\n",
    "                image_id, x, y, width, height = line.strip().split()\n",
    "                bounding_box[image_id] = [float(x), float(y), float(width), float(height)]\n",
    "        return bounding_box\n",
    "\n",
    "    def _load_part_name_mapping (self):\n",
    "        part_names = {}\n",
    "        part_names_path = os.path.join(self.root_dir, 'parts', 'parts.txt')\n",
    "        with open(part_names_path, 'r') as file:\n",
    "            for line in file:\n",
    "                part_id, part_name = line.strip().split()\n",
    "                part_names[part_id] = {}\n",
    "                part_names[part_id] = part_name\n",
    "        return part_names\n",
    "              \n",
    "    def _load_part_locs_mapping (self):\n",
    "        part_locs_mapping = {}\n",
    "        part_locs = os.path.join(self.root_dir, 'parts', 'part_locs.txt')\n",
    "        with open(part_locs, 'r') as file:\n",
    "            for line in file:\n",
    "                image_id, part_id, x, y, _ = line.strip().split()\n",
    "                if image_id not in part_locs_mapping:\n",
    "                    part_locs_mapping[image_id] = {}\n",
    "                part_locs_mapping[image_id][part_id] = [float (x), float (y)]\n",
    "        return part_locs_mapping\n",
    "\n",
    "    def _load_label_mapping (self):\n",
    "        label_mapping = {}\n",
    "        label_file = os.path.join(self.root_dir,'image_class_labels.txt')\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                image_id, label_id = line.strip().split()\n",
    "                label_mapping[image_id] = int(label_id)\n",
    "        return label_mapping\n",
    "\n",
    "    def _load_image_mapping (self):\n",
    "        image_mapping = {}\n",
    "        image_file = os.path.join(self.root_dir,'images.txt')\n",
    "        with open(image_file, 'r') as file:\n",
    "            for line in file:\n",
    "                image_id, image_name = line.strip().split()\n",
    "                image_mapping[image_id] = image_name\n",
    "        return image_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mapping)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_mapping[str(index+1)]\n",
    "        part_locs = self.part_locs_mapping.get(str(index), [])\n",
    "        image_name = self.image_mapping[str(index)]\n",
    "        bboxes = self.bounding_box_mapping.get(str(index), [])\n",
    "        #part_name = self.part_name_mapping.get(str(index),[])\n",
    "        image= Image.open(os.path.join(self.image_dir,image_name)).convert('RGB')\n",
    "        \n",
    "        # Didnt check if this task is done right, because no need to be in here currently \n",
    "        # class_names_list= []\n",
    "        # class_names = self.class_names.get(str(index), [])\n",
    "        # for int (i) in class_names:\n",
    "        #     class_names_list.append(class_names[i])\n",
    "        \n",
    "        part_name_list = []\n",
    "        part_name_dict = {}\n",
    "        for key in part_locs:\n",
    "            part_name = self.part_name_mapping.get(str(key),[])\n",
    "            part_name_dict[part_name] = part_locs[key]\n",
    "            part_name_list.append(part_name)\n",
    "\n",
    "        if self.transform:\n",
    "           image = self.transform(image)\n",
    "#        return (image, label,part_name, part_locs, bboxes, part_name_list, part_name_dict)\n",
    "        return (image, label, part_locs, bboxes, part_name_dict) #, class_names    # why has to have part_locs to display right the location?\n",
    "    # dict: 'back': [0.0, 0.0],'beak': [312.0, 182.0], 'belly': [0.0, 0.0], 'breast': [0.0, 0.0], 'crown': [186.0, 45.0],..\n",
    "    # locs: '8': [0.0, 0.0], '9': [0.0, 0.0], '10': [100.0, 221.0], '11': [183.0, 101.0], '12': [0.0, 0.0], '13': [0.0, 0.0], .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=RGB size=500x335 at 0x21F2BE433D0>, 1, {'1': [0.0, 0.0], '2': [312.0, 182.0], '3': [0.0, 0.0], '4': [0.0, 0.0], '5': [186.0, 45.0], '6': [247.0, 79.0], '7': [0.0, 0.0], '8': [0.0, 0.0], '9': [0.0, 0.0], '10': [100.0, 221.0], '11': [183.0, 101.0], '12': [0.0, 0.0], '13': [0.0, 0.0], '14': [0.0, 0.0], '15': [215.0, 194.0]}, [60.0, 27.0, 325.0, 304.0], {'back': [0.0, 0.0], 'beak': [312.0, 182.0], 'belly': [0.0, 0.0], 'breast': [0.0, 0.0], 'crown': [186.0, 45.0], 'forehead': [247.0, 79.0], 'left_eye': [0.0, 0.0], 'left_leg': [0.0, 0.0], 'left_wing': [0.0, 0.0], 'nape': [100.0, 221.0], 'right_eye': [183.0, 101.0], 'right_leg': [0.0, 0.0], 'right_wing': [0.0, 0.0], 'tail': [0.0, 0.0], 'throat': [215.0, 194.0]})\n"
     ]
    }
   ],
   "source": [
    "### for testing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = 'D:\\TorchProject\\dataset\\cub\\CUB_200_2011'\n",
    "# run dataset == __init__\n",
    "dataset = CUB(root_dir) # --> load sample to check\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False) #divide dataset by batch\n",
    "image, label, part_locs, bboxes, part_name_dict = dataset[1]\n",
    "\n",
    "print(dataset[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def visualize_bounding_box(image, part_name_dict, bboxes):\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x, y, width, height = bboxes\n",
    "    rect_pb = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "    ax.add_patch(rect_pb)\n",
    "    # Iterate over part bounding boxes and draw rectangles\n",
    "    for part_id, part_bb in part_name_dict.items():\n",
    "        x_pb, y_pb = part_bb\n",
    "        width_pb, height_pb = 5, 5  # Modify the width and height as per your requirement\n",
    "        rect_pb = patches.Rectangle((x_pb, y_pb), width_pb, height_pb, linewidth=1, edgecolor='b', facecolor='none')\n",
    "        ax.add_patch(rect_pb)\n",
    "\n",
    "        # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "root_dir = 'D:\\TorchProject\\dataset\\cub\\CUB_200_2011'\n",
    "# run dataser == __init__\n",
    "dataset = CUB(root_dir) # --> load sample to check\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False) #divide dataset by batch\n",
    "\n",
    "# call len  == __len__\n",
    "print (len(dataset))\n",
    "\n",
    "for i in range (1,10):\n",
    "    image, label, part_locs, bboxes, part_name_dict = dataset[i]\n",
    "    print (label,part_name_dict, part_locs,bboxes)\n",
    "\n",
    "    # Visualize all bounding boxes\n",
    "    visualize_bounding_box(image, part_locs, bboxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Class names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classes for embedding text \n",
    "\n",
    "root_dir = 'D:\\TorchProject\\dataset\\cub\\CUB_200_2011'\n",
    "\n",
    "CLASS_NAMES = {}\n",
    "classes_file = os.path.join(root_dir,'classes.txt')\n",
    "with open(classes_file, 'r') as file:\n",
    "    for line in file:\n",
    "        class_id, class_name = line.strip().split()\n",
    "        if class_id not in CLASS_NAMES:\n",
    "            CLASS_NAMES[class_id] = {}\n",
    "        CLASS_NAMES[class_id] = class_name\n",
    "print(CLASS_NAMES)\n",
    "\n",
    "for i in range (1,201):\n",
    "    print (CLASS_NAMES[str(i)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the Part names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the parts for embedding text \n",
    "\n",
    "root_dir = 'D:\\TorchProject\\dataset\\cub\\CUB_200_2011'\n",
    "\n",
    "PART_NAMES = {}\n",
    "parts_file = os.path.join(root_dir,'parts','parts.txt')\n",
    "with open(parts_file, 'r') as file:\n",
    "    for line in file:\n",
    "        part_id, part_name = line.strip().split()\n",
    "        if part_id not in PART_NAMES:\n",
    "            PART_NAMES[part_id] = {}\n",
    "        PART_NAMES[part_id] = part_name\n",
    "print(PART_NAMES)\n",
    "\n",
    "for i in range (1,len(PART_NAMES)+1):\n",
    "    print (PART_NAMES[str(i)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# embedding by BERT\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "### Test \n",
    "# Initialize the TextEmbedder\n",
    "text_embedder = TextEmbedder()\n",
    "\n",
    "embedded_class_names = {}\n",
    "# Example class names\n",
    "for i in range (1,len(CLASS_NAMES)+1):\n",
    "    embedded_class_names[i] = (text_embedder.embed_text(CLASS_NAMES[str(i)]))\n",
    "\n",
    "# Print the embeddings\n",
    "#print(embedded_class_names)\n",
    "\n",
    "\n",
    "embedded_part_names = {}\n",
    "for i in range (1,len(PART_NAMES)+1):\n",
    "    embedded_part_names[i] = (text_embedder.embed_text(CLASS_NAMES[str(i)]))\n",
    "    \n",
    "# Print the embeddings\n",
    "print(embedded_part_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check embedded tensor \n",
    "\n",
    "#len(embedded_class_names)\n",
    "#embedded_class_names[100]\n",
    "embedded_class_names[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize image to use ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    \"\"\"\n",
    "    Resize the input image while maintaining its aspect ratio.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image.\n",
    "        target_size (tuple): Desired target size (width, height).\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: Resized image.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    aspect_ratio = width / height\n",
    "    target_width, target_height = target_size\n",
    "\n",
    "    if width > height:\n",
    "        new_width = target_width\n",
    "        new_height = int(target_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = target_height\n",
    "        new_width = int(target_height * aspect_ratio)\n",
    "\n",
    "    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "    return resized_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the position of the points in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_points_after_resize(original_size, new_size, points):\n",
    "    \"\"\"\n",
    "    Update points' positions after resizing an image.\n",
    "    \n",
    "    Args:\n",
    "        original_size (tuple): Original image size (width, height).\n",
    "        new_size (tuple): Resized image size (width, height).\n",
    "        points (list of tuples): List of (x, y) coordinates of points.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Updated points' positions.\n",
    "    \"\"\"\n",
    "    original_width, original_height = original_size\n",
    "    new_width, new_height = new_size\n",
    "\n",
    "    updated_points = []\n",
    "    for x, y in points:\n",
    "        new_x = int(x * (new_width / original_width))\n",
    "        new_y = int(y * (new_height / original_height))\n",
    "        updated_points.append((new_x, new_y))\n",
    "\n",
    "    return updated_points\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\An\\anaconda3\\envs\\graph\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  {'pixel_values': tensor([[[[-0.9927, -0.9969, -0.9929,  ..., -0.9994, -0.9922, -0.9937],\n",
      "          [-0.9977, -0.9980, -0.9990,  ..., -0.9965, -0.9992, -0.9990],\n",
      "          [-0.9988, -0.9972, -0.9955,  ..., -0.9996, -0.9926, -0.9951],\n",
      "          ...,\n",
      "          [-0.9949, -0.9978, -0.9960,  ..., -0.9931, -0.9989, -0.9926],\n",
      "          [-1.0000, -0.9978, -0.9992,  ..., -0.9929, -0.9992, -0.9954],\n",
      "          [-0.9947, -0.9923, -0.9969,  ..., -0.9946, -0.9952, -0.9929]],\n",
      "\n",
      "         [[-0.9968, -0.9941, -0.9942,  ..., -0.9936, -0.9969, -0.9991],\n",
      "          [-0.9967, -0.9937, -0.9991,  ..., -0.9937, -0.9977, -0.9934],\n",
      "          [-0.9954, -0.9952, -0.9976,  ..., -0.9966, -0.9957, -0.9963],\n",
      "          ...,\n",
      "          [-0.9951, -0.9951, -0.9990,  ..., -0.9936, -0.9975, -0.9992],\n",
      "          [-0.9931, -0.9998, -0.9928,  ..., -0.9944, -0.9988, -0.9974],\n",
      "          [-0.9938, -0.9946, -0.9929,  ..., -0.9927, -0.9931, -0.9958]],\n",
      "\n",
      "         [[-0.9994, -0.9942, -0.9923,  ..., -0.9983, -0.9984, -0.9922],\n",
      "          [-0.9975, -0.9971, -0.9949,  ..., -0.9953, -0.9997, -0.9956],\n",
      "          [-0.9925, -0.9957, -0.9964,  ..., -0.9926, -0.9951, -0.9977],\n",
      "          ...,\n",
      "          [-0.9931, -0.9972, -0.9958,  ..., -0.9930, -0.9994, -0.9930],\n",
      "          [-0.9979, -0.9941, -0.9933,  ..., -0.9993, -0.9979, -0.9999],\n",
      "          [-0.9981, -0.9969, -0.9930,  ..., -0.9942, -0.9946, -0.9990]]]])}\n",
      "output:  ImageClassifierOutput(loss=None, logits=tensor([[-0.0904,  0.0161]]), hidden_states=(tensor([[[-3.3752e-03,  1.3522e-02, -4.8854e-01,  ..., -5.6801e-03,\n",
      "          -7.6900e-04, -7.5266e-01],\n",
      "         [ 9.8209e-02, -7.3944e-02,  7.7800e-02,  ...,  5.9340e-02,\n",
      "          -3.4518e-02, -1.4405e+00],\n",
      "         [-3.4985e-02,  4.7579e-02, -1.4736e-01,  ...,  9.5193e-02,\n",
      "           9.5757e-02, -1.3777e+00],\n",
      "         ...,\n",
      "         [-8.2583e-02,  4.3596e-02, -7.0497e-01,  ...,  5.9501e-02,\n",
      "           1.9862e-02, -1.4481e+00],\n",
      "         [-1.4377e-01,  1.1498e-01, -6.0000e-01,  ...,  1.0259e-01,\n",
      "           8.0886e-02, -1.5754e+00],\n",
      "         [-6.5507e-02,  8.3904e-02, -3.1681e-01,  ..., -2.1756e-02,\n",
      "          -8.6474e-02, -1.5365e+00]]]), tensor([[[-0.2435, -0.0417, -0.6700,  ...,  0.1696,  0.0664, -0.7087],\n",
      "         [ 0.6285,  0.0091,  0.6932,  ..., -1.0060, -0.0680, -2.1986],\n",
      "         [ 0.4539,  0.2850,  0.3764,  ..., -0.8526, -0.0777, -1.9526],\n",
      "         ...,\n",
      "         [ 0.2320,  0.0773, -0.5179,  ..., -1.0321, -0.5097, -2.2604],\n",
      "         [ 0.2009,  0.0998, -0.2569,  ..., -0.8671, -0.3801, -2.6182],\n",
      "         [ 0.4846, -0.1815, -0.1404,  ..., -1.0403, -0.6490, -2.5653]]]), tensor([[[-0.2708, -0.0095, -0.6487,  ...,  0.4091,  0.4733, -0.5471],\n",
      "         [ 0.4573,  1.3979,  0.6171,  ...,  0.3538,  0.6454, -2.2340],\n",
      "         [ 0.1147,  1.2965,  0.2766,  ...,  0.4497,  1.2670, -2.0909],\n",
      "         ...,\n",
      "         [-0.1829,  0.8734, -0.9212,  ...,  0.3064,  0.7295, -2.4144],\n",
      "         [-0.1899,  1.0370, -0.7803,  ...,  0.3900,  0.7122, -3.0481],\n",
      "         [ 0.3006,  1.1608, -0.7660,  ...,  0.3780, -0.1656, -2.5968]]]), tensor([[[-0.6158,  0.9425, -0.4653,  ...,  1.5051,  1.3476, -0.3356],\n",
      "         [ 2.3993,  1.3415,  0.7154,  ...,  0.2009,  1.1464, -0.3846],\n",
      "         [ 1.4223,  1.1374,  0.5323,  ...,  0.1312,  2.9224, -0.2112],\n",
      "         ...,\n",
      "         [ 1.4517,  1.0764, -1.6620,  ...,  1.3842,  2.4548, -0.2988],\n",
      "         [ 1.2159,  0.8037, -1.7469,  ...,  1.3384,  2.2343, -1.0161],\n",
      "         [ 2.3067,  1.2243, -2.0627,  ...,  1.4632,  0.1372, -0.3046]]]), tensor([[[ 0.1212,  0.8404, -0.5104,  ...,  2.5194,  1.0474, -0.8096],\n",
      "         [ 1.9358,  1.5009,  1.4255,  ...,  0.1495,  1.4161, -0.1391],\n",
      "         [ 0.4860,  1.3484,  1.2800,  ...,  0.1789,  3.8322, -0.8432],\n",
      "         ...,\n",
      "         [ 1.1151,  0.5763, -1.1768,  ...,  1.2543,  2.9314, -0.7151],\n",
      "         [ 1.1213,  0.4784, -1.3872,  ...,  1.2360,  2.7866, -1.3113],\n",
      "         [ 2.8826,  1.1367, -1.5807,  ...,  1.0869, -0.3767,  0.7150]]]), tensor([[[-0.1463,  1.2639, -1.0217,  ...,  2.4681,  0.8579,  1.7999],\n",
      "         [ 1.8058,  1.2356,  0.9158,  ...,  2.1726,  1.5384,  3.6767],\n",
      "         [ 0.5741,  1.6045,  1.3408,  ...,  1.5141,  4.7556,  2.2385],\n",
      "         ...,\n",
      "         [ 0.8883,  1.2846, -1.6126,  ...,  2.3054,  3.9724,  2.0347],\n",
      "         [ 0.8972,  1.2325, -1.8355,  ...,  2.1855,  3.8425,  1.8269],\n",
      "         [ 2.4964,  1.1679, -1.7552,  ...,  2.7100, -0.3197,  4.6083]]]), tensor([[[-0.3723,  1.3460, -2.0504,  ...,  3.1697, -0.0235,  1.9694],\n",
      "         [ 2.6146,  2.1995, -0.7061,  ...,  2.9157,  0.8044,  3.9237],\n",
      "         [ 1.4472,  2.9111,  0.0370,  ...,  2.4922,  4.5229,  2.8028],\n",
      "         ...,\n",
      "         [ 1.9753,  2.3671, -2.3004,  ...,  3.1991,  4.1199,  2.1673],\n",
      "         [ 2.0452,  2.2022, -2.7438,  ...,  3.1855,  3.7948,  1.9719],\n",
      "         [ 3.4554,  1.5969, -2.7851,  ...,  2.8890, -0.5350,  4.3458]]]), tensor([[[-0.5080,  0.5026, -3.5310,  ...,  3.4180, -0.7439,  1.9730],\n",
      "         [ 2.2507,  1.1447, -1.8812,  ...,  2.3049,  0.5546,  2.1187],\n",
      "         [ 1.1713,  2.7712, -1.2021,  ...,  2.3703,  3.2781,  2.3855],\n",
      "         ...,\n",
      "         [ 1.3798,  2.1476, -2.9939,  ...,  2.5992,  3.3617,  2.1963],\n",
      "         [ 1.4375,  1.7407, -3.4656,  ...,  2.3726,  3.2235,  1.8182],\n",
      "         [ 2.8312,  0.6285, -4.0430,  ...,  2.0693,  0.0217,  2.8251]]]), tensor([[[ 1.3662,  1.1314, -4.0380,  ...,  2.7965, -1.9880,  1.4069],\n",
      "         [ 3.2789,  1.6441, -2.3023,  ...,  2.7378, -0.1849,  2.0410],\n",
      "         [ 1.8708,  3.1135, -0.8518,  ...,  2.6428,  2.7315,  1.5477],\n",
      "         ...,\n",
      "         [ 1.9903,  1.9623, -2.9472,  ...,  3.1367,  2.7440,  1.3574],\n",
      "         [ 1.9950,  1.6279, -3.5833,  ...,  3.2455,  2.5616,  0.8726],\n",
      "         [ 3.1257,  1.1965, -4.3358,  ...,  3.1684, -0.5226,  2.3193]]]), tensor([[[ 2.9831,  2.4404, -4.2090,  ...,  2.1038, -0.9211,  2.0655],\n",
      "         [ 5.4800,  1.8132, -4.3913,  ...,  3.3492,  1.0637,  3.4779],\n",
      "         [ 3.1597,  3.0649, -2.0764,  ...,  2.8088,  3.4454,  2.1959],\n",
      "         ...,\n",
      "         [ 3.1408,  2.1871, -4.8566,  ...,  3.6363,  3.8056,  2.7713],\n",
      "         [ 3.2082,  2.0600, -5.6773,  ...,  3.3382,  3.2486,  2.4755],\n",
      "         [ 5.0711,  2.6567, -6.6286,  ...,  3.1213,  0.1267,  4.5051]]]), tensor([[[ 2.7367,  2.7912, -4.6390,  ...,  3.5539, -3.4548,  0.9603],\n",
      "         [ 7.9361,  1.5142, -6.8936,  ...,  6.7866, -4.1269,  4.0085],\n",
      "         [ 5.6822,  3.1637, -2.9880,  ...,  3.6163,  0.0346,  2.9531],\n",
      "         ...,\n",
      "         [ 6.1638,  3.1945, -7.5683,  ...,  4.0442,  0.0446,  1.4039],\n",
      "         [ 5.9719,  2.2977, -8.5752,  ...,  4.8371, -0.3434,  1.1195],\n",
      "         [ 7.2224,  3.5373, -8.6425,  ...,  6.4545, -4.6475,  2.3337]]]), tensor([[[  5.6158,   4.3592,  -6.9591,  ...,   2.2806,  -2.0875,   2.0054],\n",
      "         [ 13.5070,   2.9433,  -7.2047,  ...,   2.4270,   0.3140,   4.7819],\n",
      "         [ 13.8967,   5.8462,  -1.8455,  ...,   0.9022,   2.7574,   3.3272],\n",
      "         ...,\n",
      "         [ 11.7808,   4.5638,  -9.1123,  ...,   2.6109,   3.0441,   2.1024],\n",
      "         [ 11.1533,   4.3788, -10.3881,  ...,   2.4236,   2.6983,   1.5204],\n",
      "         [ 10.3826,   5.7570, -12.2747,  ...,   5.4111,  -0.8188,   0.4328]]]), tensor([[[ 12.0504,   6.0297, -14.4576,  ...,   1.2018,   1.0981,   4.8775],\n",
      "         [ 15.4533,   3.7935,  -9.1469,  ...,   1.4233,   2.8842,   6.2785],\n",
      "         [ 16.0416,   6.2984,  -3.9666,  ...,   0.2100,   4.8344,   5.7158],\n",
      "         ...,\n",
      "         [ 15.0758,   5.2588, -12.5752,  ...,   1.1975,   6.5721,   3.0267],\n",
      "         [ 14.2664,   5.0682, -13.7643,  ...,   0.7321,   6.3030,   2.3633],\n",
      "         [ 12.8486,   6.7197, -14.5515,  ...,   4.4248,   2.5708,   2.5883]]])), attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "class ViTHandler:\n",
    "    def __init__(self, variant='google/vit-base-patch16-224-in21k'):\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(variant)\n",
    "        self.model = ViTForImageClassification.from_pretrained(variant, output_hidden_states = True)\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        inputs = self.feature_extractor(images=image, return_tensors='pt')\n",
    "        print (\"input: \", inputs)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            print(\"output: \", outputs)\n",
    "        embeddings = outputs.hidden_states[-1] # take the last hidden state - why? dont know\n",
    "        return embeddings\n",
    "\n",
    "### test\n",
    "# Initialize the ViTHandler with a specific variant\n",
    "vit_handler = ViTHandler(variant='google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# TURN IMAGE TO TENSOR (replace this with your image data) \n",
    "image_tensor = torch.rand(1, 3, 224, 224)  # Batch size, channels, height, width\n",
    "\n",
    "# Process the image and obtain visual embeddings\n",
    "embeddings = vit_handler.process_image(image_tensor)\n",
    "#print(embeddings.shape)\n",
    "\n",
    "# Now you can use these embeddings in your Graph Vision Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Construction\n",
    "- Node (1 -> 15)\n",
    "- Node (16 -> 216)\n",
    "- Edges (init)\n",
    "\n",
    "- load the class names from classes.txt file\n",
    "- embedding each class name (each node) \n",
    "- put each embedded name into each node (from 16 to 216) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph construction\n",
    "import networkx as nx\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class Graph_construct:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "  \n",
    "\n",
    "    def add_nodes(self, part_locs):\n",
    "        # Add nodes from 1st to 15th with their attributes\n",
    "        for i in range(1, 16):\n",
    "            x, y = part_locs[str(i)] # Replace with actual position (x, y) data if available\n",
    "            title = f\"Node {i}\"\n",
    "            image_tensor = torch.rand(16, 16)  # Replace with actual image tensor data if available\n",
    "\n",
    "            node_attributes = {\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"title\": title,\n",
    "                \"image\": image_tensor ## wait for ViT to split\n",
    "            }\n",
    "\n",
    "            self.graph.add_node(i, **node_attributes)\n",
    "\n",
    "        # Add nodes from 16th to the rest with their class_name attribute\n",
    "        # num_classes = 200  # Replace with the actual number of classes in the dataset\n",
    "\n",
    "        text_embedder = TextEmbedder()\n",
    "\n",
    "        embedded_class_names = {}\n",
    "        for i in range (1,len(CLASS_NAMES)+1):\n",
    "            embedded_class_names[i] = (text_embedder.embed_text(CLASS_NAMES[str(i)]))\n",
    "\n",
    "        for i in range(16, 216):\n",
    "            node_attributes = {\"class_name\": embedded_class_names[(i - 15)]}\n",
    "            self.graph.add_node(i, **node_attributes)\n",
    "\n",
    "    def calculate_edges(self):\n",
    "        # Calculate edges for nodes from 1st to 15th based on distance between (x, y) positions\n",
    "        for i in range(1, 16):\n",
    "            for j in range(i + 1, 16):\n",
    "                x1, y1 = self.graph.nodes[i][\"x\"], self.graph.nodes[i][\"y\"]\n",
    "                x2, y2 = self.graph.nodes[j][\"x\"], self.graph.nodes[j][\"y\"]\n",
    "                distance = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "                self.graph.add_edge(i, j, weight=distance)\n",
    "\n",
    "        # Set edges from nodes 16th to the rest to others nodes = 1 for the initial step\n",
    "        for i in range(16, 216):\n",
    "            for j in range(1, 16):\n",
    "                self.graph.add_edge(i, j, weight=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = 'D:\\TorchProject\\dataset\\cub\\CUB_200_2011'\n",
    "    # run dataser == __init__\n",
    "    dataset = CUB(root_dir)\n",
    "    \n",
    "    graph_construct = Graph_construct()\n",
    "\n",
    "    graph_construct.add_nodes(dataset[2][2]) # dataset -> image, label, part_locs, \n",
    "    graph_construct.calculate_edges()\n",
    "\n",
    "    # You can access the graph object with all nodes and edges\n",
    "    G = graph_construct.graph\n",
    "    print(G.nodes(data=True))\n",
    "    print(G.edges(data=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
    "        return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Parts_name should be embedded to put in the node or be used as the label (for level 1: detect the parts of the birds)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN \n",
    "can concate or combine (plus) the position, the embedding (output of last hidden state from ViT), and the embedded text of parts/classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
